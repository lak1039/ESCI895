{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9177bf50",
   "metadata": {},
   "source": [
    "## ESCI 895 Final Project\n",
    "Lauren K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db8714",
   "metadata": {},
   "source": [
    "#### Project Summary:\n",
    "    The goal of this project is to determine the discharge associated with the 2, 5, 10, 25, 50, and 100-year floods on the Lamprey and Sugar Rivers in New Hampshire. Two methods will be compared, one using annual maximum discharge records and the other using instaneous discharge data. The first method will fit the data to a Gumbel distribution. The second with use the General Pareto distribtuion (GPD) with a Peaks-Over-Threshold (POT) method.\n",
    "    \n",
    "    The annual maxiumum data has a longer record period for both rivers but only takes into account one large flood per year. While unlikely, extreme floods that happen within the same year will not be accounted for in the analysis. The instaneous data, using a peaks over threshold method to select only large floods, does not have this issue. The record length for this data set is much shorter, which may not provide accurate estimates for return periods of floods longer than the record itself, like the 25-year, 50-year, and 100-year floods. \n",
    "    \n",
    "    The instananoeus data also requires more processing to select the maximum values from individual floods events, rather than analyzing multiple values measured duing the same flood event that may be above the threshold. A threshold value will also need to be determined. Other studies have recommend using a threshold that is equal to the magnitude of the discharge asscoiated 1-year to 2-year flood.\n",
    "    \n",
    "#### Study Sites: UPDATE TO INCLUDE BOTH INSTANEOUS AND ANNUAL DATA SETS AND RECORD LENGTHS\n",
    "##### Lamprey River:\n",
    "\tThe Lamprey River is located in southeastern New Hampshire. It 47 miles long, flowing from Northwood, NH to Newmarket, NH where it empties into the Great Bay. The Lamprey River watershed is 212 square miles. Daily discharge data was recorded at the USGS 01060003 station located in Newmarket, NH. This station has 86 years of consecutive daily discharge data (USGS). The gauge station is located directly upstream of Packerâ€™s Falls. The drainage area upstream of the flow gauge is 185 square miles. The Lamprey River has some regulation by Pawtuckaway and Mendums Ponds. Occasionally, water from the river is diverted for the municipal supply of Durham. Newmarket, NH has a humid continental climate with hot summers and cold winters. Newmarket, NH has an average annual rainfall of 48 inches per year and snowfall of 53 inches per year.\n",
    "\n",
    "Add a figure\n",
    "\n",
    "##### Sugar River:\n",
    "\tThe Sugar River is located in western New Hampshire. It originates as an outlet of Lake Sunapee in Sunapee, NH, and heads west through Newport and Claremont, NH before emptying into the Connecticut River. The River is 27 miles long. The USGS gauge used in this project is located in West Claremont, NH upstream of a small waterfall. The gauge station is a few miles downstream of a large waterfall located in Claremont, NH. The gauge has a station identification number of USGS 01152500. The drainage basin draining to the location of this gauge is 269 square miles (USGS). The Sugar River is regulated by Sunapee Lake and occasionally diverted by mills upstream. This station has recorded 93 years of daily discharge data. Claremont, NH has a humid continental climate with a yearly average of 41.3 in of rain and 74.7 in of snowfall.\n",
    "\n",
    "Add a figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383cd502",
   "metadata": {},
   "source": [
    "#### Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea020228",
   "metadata": {},
   "source": [
    "    Data was analyzed using python ____ (say edition used?). ____ were imported to analyze the data in following steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d60cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Import libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "import pandas as pd\n",
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2139f3d",
   "metadata": {},
   "source": [
    "    Annual maximum discharge and instaneous discharge data sets were retrieved from USGS databases for each gage and loaded into python jupyter. Excess data was removed, leaving only the timestamps and discharge values. Missing or corrupt data was replaced using a linear interpolation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3f98795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Specify inputs\n",
    "filenames = ['hourlyqlamprey.txt', 'SugarInstant.txt', 'annualn.txt']\n",
    "rivers = [\"Lamprey River\", \"Sugar River\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e6ae9",
   "metadata": {},
   "source": [
    "    First, data from the annual maximum discharge values were analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8bff49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4be3b07",
   "metadata": {},
   "source": [
    "    Second, instaneous discharge data was analyzed using the POT method and General Pareto Distribution (GPD). First, A threshold value needs to be selected for the POT method. Then values below the threshold can be removed for the rest of this analysis. \n",
    "\n",
    "***Data is both in 1 hour and 15 min increments so need to remove data not on hour increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c99d14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Part 1\n",
    "def GEV(file, threshold):\n",
    "    #Load discharge file into dfpeak and format\n",
    "    dfpeak = pd.read_csv(filenames[0], delimiter=\"\\t\", comment='#', header=1, parse_dates=['20d'], na_values = (9999, 999, 997, \"Eqp\"))\n",
    "    #Rename columns\n",
    "    dfpeak = dfpeak.rename(columns={\"20d\": \"DATE\",\"14n\": \"discharge_cfs\"})\n",
    "    #Set date column as index\n",
    "    dfpeak =dfpeak.set_index('DATE')\n",
    "    #Remove not needed columns\n",
    "    dfpeak = dfpeak[['discharge_cfs']]\n",
    "    #Fill nan values through linear interpolation\n",
    "    dfpeak.interpolate(method = 'linear',inplace = True)\n",
    "    \n",
    "    \n",
    "    #POT \n",
    "    #select discharges greater than threshold\n",
    "    #threshold = 250\n",
    "    #Drop all values below the threshold which are not needed for the extreme events analysis.\n",
    "    dfpeak.drop(dfpeak[dfpeak['discharge_cfs'] < threshold].index, inplace = True)\n",
    "    #Resample data to daily max daily data.\n",
    "    #This accounts for uneven sampling intervals for data sets with hourly, 30 minute, and 15 minute measurement intervals.\n",
    "    #This also helps simply analysis by lessening the amount of data values. This can be done as multiple extreme flood events wouldn't likely happen in the same day).\n",
    "    dfpeak = dfpeak.resample('D').max()\n",
    "    #Drop values below threshold.\n",
    "    dfpeak = dfpeak.dropna()\n",
    "    \n",
    "    #Still need to select ind. floods events\n",
    "    #something like if the row before is nan, and row below\n",
    "    #is not then select max of all values until next nan?\n",
    "    #But what if there are two major floods and discharge\n",
    "    #never drops below threhold between them?\n",
    "    \n",
    "    #Running frequency analysis\n",
    "    #Make a datframe to fill in later\n",
    "    interp = np.array([2,5,10,25,50,100,200,500,1000])\n",
    "    dfinterp = pd.DataFrame(interp, columns=['Return Period (yrs)'])\n",
    "    dfinterp['EP'] = 1/dfinterp['Return Period (yrs)']\n",
    "    dfinterp['1 - EP'] = 1 - dfinterp['EP']\n",
    "    #Take mean and std of peak annual discharge data\n",
    "    peak_mean = dfpeak['discharge_cfs'].mean()\n",
    "    peak_std = dfpeak['discharge_cfs'].std()\n",
    "    \n",
    "    #Rank and order data\n",
    "    count = dfpeak['discharge_cfs'].count()\n",
    "    dfpeak = dfpeak.sort_values('discharge_cfs', ascending = True)\n",
    "    dfpeak['rank'] = dfpeak['discharge_cfs'].rank(ascending= False)\n",
    "    dfpeak = dfpeak.sort_values(by = 'rank', ascending = True)\n",
    "    #Calculate L moments\n",
    "    num = len(dfpeak)\n",
    "    dfpeak['b1'] = ((num-dfpeak['rank'])/(num*(num-1)))*dfpeak['discharge_cfs']\n",
    "    dfpeak['b2'] = (((num-dfpeak['rank'])*(num-dfpeak['rank']-1))/(num*(num-1)*(num-2)))*dfpeak['discharge_cfs']\n",
    "    B1 = sum(dfpeak['b1'])\n",
    "    B2 = sum(dfpeak['b2'])\n",
    "    \n",
    "    lamda1 = np.mean(dfpeak['discharge_cfs'])\n",
    "    lamda2 = 2*B1-lamda1\n",
    "    lamda3 = 6*B2-6*B1+lamda1\n",
    "    skew = lamda3/lamda2\n",
    "    \n",
    "    #GEV - need to change to be GPD\n",
    "    c = 2*lamda2/(lamda3+3*lamda2)-np.log(2)/np.log(3)\n",
    "    k = 7.859*c+2.9554*c**2\n",
    "    alpha = (k*lamda2)/(gamma(1+k)*(1-2**(-k)))\n",
    "    squiggle = lamda1 + (alpha/k)*(math.gamma(1+k)-1)\n",
    "    print(c , k , alpha, squiggle)\n",
    "    dfinterp['GEV'] = squiggle+(alpha/k)*(1-(-np.log(dfinterp['1 - EP']))**k)\n",
    "    #Create figures to show this\n",
    "    print(dfinterp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99d0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30b44687",
   "metadata": {},
   "source": [
    "##### GEV analysis for the Lamprey River:\n",
    "\n",
    "Selecting a threshold for the Lamprey River... Selected 250 cfs, which is the 1.5-year flood estimate from the previous analysis using annual maximum data and the GEV??? method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c48d0cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05500071989807398 -0.4233103386428873 179.62098848316572 410.9113332392692\n",
      "   Return Period (yrs)     EP  1 - EP          GEV\n",
      "0                    2  0.500   0.500   482.126415\n",
      "1                    5  0.200   0.800   787.248687\n",
      "2                   10  0.100   0.900  1086.627455\n",
      "3                   25  0.040   0.960  1629.902589\n",
      "4                   50  0.020   0.980  2199.864163\n",
      "5                  100  0.010   0.990  2960.962434\n",
      "6                  200  0.005   0.995  3979.477505\n",
      "7                  500  0.002   0.998  5875.241837\n",
      "8                 1000  0.001   0.999  7884.952900\n"
     ]
    }
   ],
   "source": [
    "GEV(filenames[0], 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f542930",
   "metadata": {},
   "source": [
    "##### GEV analysis for the Sugar River:\n",
    "\n",
    "Selecting a threshold for the Sugar River... The selected threshold values was 400?? cfs, which is the 1.5-year flood estimate from the previous analysis using annual maximum data and the GEV??? method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2263df51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.061030092473016384 -0.46862760056319297 183.8193632897032 563.665800321274\n",
      "   Return Period (yrs)     EP  1 - EP           GEV\n",
      "0                    2  0.500   0.500    637.169812\n",
      "1                    5  0.200   0.800    963.615243\n",
      "2                   10  0.100   0.900   1297.480532\n",
      "3                   25  0.040   0.960   1927.464694\n",
      "4                   50  0.020   0.980   2613.131914\n",
      "5                  100  0.010   0.990   3558.271289\n",
      "6                  200  0.005   0.995   4863.649709\n",
      "7                  500  0.002   0.998   7385.339008\n",
      "8                 1000  0.001   0.999  10156.330859\n"
     ]
    }
   ],
   "source": [
    "GEV(filenames[1], 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7c1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show a figure of the distribution... \n",
    "#maybe highlight/show all the extreme values in different color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c787c1",
   "metadata": {},
   "source": [
    "***Ask Anne about how to talk about steps done within the function (like it as a function because its easy to run for different data sets but isn't conducive to juptyer format for explanation then code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a49c85e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db9ecd06",
   "metadata": {},
   "source": [
    "#### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab3561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display dfinterp data frames... make one that shows values for both distributions.\n",
    "\n",
    "#For GPD can show CDF and PDF graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78433cae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635aaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
